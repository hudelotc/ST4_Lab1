{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FpMYK1xZLWmH"
   },
   "source": [
    "# Recherche d'Information et traitement de données massives\n",
    "\n",
    "# Lab 1 : Indexation - Représentation d'un document et d'un corpus\n",
    "\n",
    "\n",
    "L'objectif de cette séance est de mettre en oeuvre les premiers concepts clés de la **Recherche d'Information** que sont l'indexation et l'index inversé. Nous travaillerons sur la collection [TIME](http://ir.dcs.gla.ac.uk/resources/test_collections/time/). Cette collection est une petite collection qui est constituée d'articles du magazine TIME et qui est disponible dans le répertoire [Data](./Data/Time). Plus précisemment, cette collection est constituée des fichiers suivants :\n",
    "+ Le fichier [TIME.ALL](./Data/Time/TIME.ALL) qui contient l'ensemble des articles sous la forme d'un seul fichier texte. Chaque article est identifié dans le fichier par une chaine de caractères du type `*TEXT XXX PAGE XXX`. Le fichier commence avec la chaîne de caractères `*TEXT 017 01/04/63 PAGE 020` et le dernier article correspond à la chaine de caractères `*TEXT 563 12/27/63 PAGE 024`. La chaîne de caractères `*STOP` indique  la fin du fichier. La collection contient en tout 423 articles. Dans la suite, les documents seront identifiés par un identifiant unique qui est le numero présent dans la chaîne `*TEXT XXX PAGE XXX`. Par exemple, l'article préfixé par `*TEXT 563 12/27/63 PAGE 024` aura comme identifiant `563`\n",
    "+ Le fichier [TIME.QUE](./Data/Time/TIME.QUE) qui contient un ensemble de requêtes exprimées en langage naturel. Chaque requête est identifiée dans le fichier par la chaîne de caractères ` *FIND      ID`\n",
    "+ Le fichier [TIME.REL](./Data/Time/TIME.REL) qui contient les jugements de pertinence exhaustifs pour chaque requête. Par exemple la ligne `1  268 288 304 308 323 326 334` doit être interprétée de la manière suivante : les documents pertinents de la collection pour la requête `1` sont les documents `268 288 304 308 323 326` et `334` \n",
    "+ Le fichier [TIME.STP](./Data/Time/TIME.STP) est une liste des stop-words à considérer dans cette collection.\n",
    "\n",
    "\n",
    "\n",
    "### Etape 1 : Prise en main des données\n",
    "\n",
    "Dans un premier temps, le travail consistera en une prise en main des données. En particulier, il s'agira de lire le fichier [TIME.ALL](./Data/Time/TIME.ALL) et de stocker la collection sous la forme d'un dictionnaire python (`dict`) avec comme **clé** l'identifiant unique de chaque article et en **valeur** une liste (`list`) des tokens de chaque article. Pour l'étape de segmentation (ou *tokenization*), on considérera les espaces et les caractères de ponctuation comme séparateurs et on fera appel à la bibliothèque [nltk](https://www.nltk.org) et en particulier à son module [tokenize](https://www.nltk.org/api/nltk.tokenize.html). La commande ci-dessous vous permet d'installer cette bibliothèque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5628,
     "status": "ok",
     "timestamp": 1557235900720,
     "user": {
      "displayName": "Tami Myriam",
      "photoUrl": "",
      "userId": "04534705195482337784"
     },
     "user_tz": -120
    },
    "id": "eoWnetcqLWmL",
    "outputId": "af0501da-a5c0-404c-e7fc-bea10995fe3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.7/site-packages (3.4)\n",
      "Requirement already satisfied: singledispatch in /opt/anaconda3/lib/python3.7/site-packages (from nltk) (3.4.0.3)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from nltk) (1.11.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r-R7Ey38LWmW"
   },
   "source": [
    "Importer la bibliothèque nltk et télécharger les jeux de données et modules nécéssaires pour le traitement du langage naturel en executant les deux commandes ci-dessous. **Attention la commande `nltk.download()` peut prendre un peu de temps et ouvre une fenêtre pop-up vous invitant à sélectionner l'ensemble des corpus utiles pour votre application (ici all) puis à lancer le téléchargement** comme montré ci-dessous.\n",
    "\n",
    "<img src=\"./Images/nltk.png\" width=\"500\" height=\"500\" />\n",
    "\n",
    "**Remarque : Lorsque le caractère `*` s'affiche dans les crochets de l'`Entrée [ ]` de la cellule de code, cela signifie que l'execution est en cours. Il est inutile d'insister en re-cliquant dessus. Il vous faut simplement patienter.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "RFhukdAALWmY",
    "outputId": "f3f88fe6-ceba-4cdc-ce03-023171070091"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DWXzDmwZLWmf"
   },
   "source": [
    "#### Lecture des données\n",
    "\n",
    "Ecrire une fonction `loadData(filename)` qui charge le fichier [TIME.ALL](./Data/Time/TIME.ALL), stocke et retourne son contenu sous la forme d'un dictionnaire python (`dict`) avec comme **clé** l'identifiant unique de chaque article et en **valeur** le contenu de l'article sous la forme d'une chaine de caractères. Pour rappel, nous prendrons comme identifiant le numéro indiqué dans le fichier. \n",
    "\n",
    "Par exemple, l'article préfixé par `*TEXT 563 12/27/63 PAGE 024` aura comme identifiant `563`. Il y a parfois des trous dans la numérotation. \n",
    "\n",
    "Pour faire cela, il vous faudra donc parser votre fichier en prenant en compte sa structure décrite ci-dessus. Vous pourrez par exemple utiliser des [expressions régulières](https://docs.python.org/3/library/re.html) pour détecter la fin du corpus ou le début d'un nouvel article. Une bon tutoriel concernant les expressions régulières est disponible [ici](http://www.xavierdupre.fr/app/teachpyx/helpsphinx/c_regex/regex.html). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour vous aider, nous vous donnons ici la méthode pour détecter la fin du fichier avec des expressions régulières. En particulier, nous utilisons l'expression régulière `r\"\\*STOP\"` qui signifie ici que l'on recherche le motif `*STOP` dans une chaîne de caractères. Le `r` placé en début de l'expression régulière  permet de considérer l’antislash `\\` comme un caractère normal. On peut tout à fait s'en passer ici. L'exemple ci-dessous vous en présente une illustration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "line = \"*STOPa\"\n",
    "print(re.match(r\"\\*STOP\\a\",line)) # False\n",
    "line = \"*STOP\\a\"\n",
    "print(re.match(r\"\\*STOP\\a\",line)) # True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayez de trouver l'expression régulière permettant la détection du début d'un article, tester la et utiliser la ainsi que la précédente pour compléter la fonction ci-dessous demandée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HWxIJi5PLWmg"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def loadData(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        corpus = {}\n",
    "        # A completer\n",
    "        return(corpus)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWn1wT2QLWml"
   },
   "source": [
    "Appliquer et tester votre fonction au fichier [TIME.ALL](./Data/Time/TIME.ALL) en executant le code ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jOA3ChWOLWmm",
    "outputId": "fed85f27-f911-4f44-c6db-6f905e6933fe"
   },
   "outputs": [],
   "source": [
    "corpus = loadData('./Data/Time/TIME.ALL')\n",
    "print(corpus['563'])\n",
    "print(corpus['563'] == 'UNITED NATIONS POTENT PYGMY IN MANHATTAN LAST WEEK, THE U.N .GREW TO A TOTAL OF 113 MEMBERS WITH THE ADMISSION OF THE NEWLYINDEPENDENT STATES OF ZANZIBAR AND KENYA . ZANZIBAR CONSISTS OF TWOSMALL ISLANDS IN THE INDIAN OCEAN, WITH A TOTAL POPULATION OF 310,000,OR ABOUT THAT OF OMAHA . NEVERTHELESS, ZANZIBAR HAS ONE VOTE IN THEGENERAL ASSEMBLY, AND IS THUS EQUAL IN VOTING POWER WITH SUCH NUCLEARGIANTS AS THE SOVIET UNION AND THE U.S .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1pQWEg2aLWmr"
   },
   "source": [
    "### Etape 2 : Extraction du vocabulaire d'indexation.\n",
    "\n",
    "Dans cette étape, vous allez mettre en oeuvre la chaîne de traitements vue dans le cours 1 et rappelée sur la figure ci-dessous.\n",
    "\n",
    "<img src=\"./Images/indexationschema.png\" width=\"500\" height=\"500\" />\n",
    "\n",
    "Il s'agira donc de mettre en oeuvre les étapes suivantes :\n",
    "    1. Segmentation d'un texte en mots (étape de tokenization)\n",
    "    2. Filtrage des mots vides\n",
    "    3. Lemmatisation ou Racinisation des termes\n",
    "    4. Extraction des mots uniques\n",
    "\n",
    "**Différentes approches seront testées pour plusieurs de ces étapes.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tz964qo2LWms"
   },
   "source": [
    "#### 1 . Segmentation d'un texte en mots\n",
    "\n",
    "Vous allez maintenant écrire un programme permettant de segmenter un article en mots ou tokens. On considèrera que l'article est fourni sous la forme d'une chaîne de caractères `text`. On testera ici différentes approches de segmentation dont :\n",
    " + une approche simple avec la méthode [`split()`](https://www.w3schools.com/python/ref_string_split.asp) de python ;\n",
    " + l'approche de segmentation en mots avec la fonction [`word_tokenize()`](https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/) de la bibliothèque nltk ;\n",
    " + une autre approche de segmentation que vous choisirez parmi les approches disponibles dans la bibliothèque [ntlk](https://www.nltk.org/api/nltk.tokenize.html). Par exemple, le `RegexpTokenizer` qui se base sur les expressions régulières peut être intéressant pour détecter les dates ou nombres dans les articles.\n",
    "\n",
    "Dans tous les cas, vos fonctions `article_tokenize_simple(text)`, `article_word_tokenize(text)` et `article_tokenize_other(text)`prendront en paramètre une chaine de caractères `text` et retourneront la liste des mots (ou *tokens*) de la chaîne `text`.\n",
    "\n",
    "Nous considérerons qu'une bonne segmentation :\n",
    " + ne considérera pas les caractères de ponctuations comme des mots ;\n",
    " + permettra de récupérer les dates ou les chiffres ;\n",
    " + permettra de garder les abbréviations d'interêt type `U.S`.\n",
    "\n",
    "\n",
    "Selon la méthode de segmentation utilisée, l'amélioration de la segmentation pourra être faite a posteriori par exemple avec du filtrage.\n",
    "\n",
    "**Approche de segmentation simple**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jZGQDARnLWmu"
   },
   "outputs": [],
   "source": [
    "def article_tokenize_simple(text):\n",
    "    if type(text)!= str:\n",
    "        raise Exception(\"The function takes a string as input data\")\n",
    "    else:\n",
    "        # A COMPLETER\n",
    "        return tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdoCjjwgoL9n"
   },
   "source": [
    "Tester votre méthode sur l'article donné ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q6h9crMuoL9x"
   },
   "outputs": [],
   "source": [
    "text = 'UNITED NATIONS POTENT PYGMY IN MANHATTAN LAST WEEK, THE U.N .GREW TO A TOTAL OF 113 MEMBERS WITH THE ADMISSION OF THE NEWLYINDEPENDENT STATES OF ZANZIBAR AND KENYA . ZANZIBAR CONSISTS OF TWOSMALL ISLANDS IN THE INDIAN OCEAN, WITH A TOTAL POPULATION OF 310,000,OR ABOUT THAT OF OMAHA . NEVERTHELESS, ZANZIBAR HAS ONE VOTE IN THEGENERAL ASSEMBLY, AND IS THUS EQUAL IN VOTING POWER WITH SUCH NUCLEARGIANTS AS THE SOVIET UNION AND THE U.S .'\n",
    " \n",
    "# A completer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UNHOLAH7oL-K"
   },
   "source": [
    "**Approche de segmentation avec la fonction `word_tokenize()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txlINJ5aoL-X"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def article_word_tokenize(text):\n",
    "    if type(text)!= str:\n",
    "        raise Exception(\"The function takes a string as input data\")\n",
    "    else:\n",
    "        # A COMPLETER\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wf6G8C7JoL-u"
   },
   "source": [
    "Tester votre méthode sur l'article donné ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bQgbDndaoL-3"
   },
   "outputs": [],
   "source": [
    "text = 'UNITED NATIONS POTENT PYGMY IN MANHATTAN LAST WEEK, THE U.N .GREW TO A TOTAL OF 113 MEMBERS WITH THE ADMISSION OF THE NEWLYINDEPENDENT STATES OF ZANZIBAR AND KENYA . ZANZIBAR CONSISTS OF TWOSMALL ISLANDS IN THE INDIAN OCEAN, WITH A TOTAL POPULATION OF 310,000,OR ABOUT THAT OF OMAHA . NEVERTHELESS, ZANZIBAR HAS ONE VOTE IN THEGENERAL ASSEMBLY, AND IS THUS EQUAL IN VOTING POWER WITH SUCH NUCLEARGIANTS AS THE SOVIET UNION AND THE U.S .'\n",
    " \n",
    "# A completer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jhs2AfDDoL_V"
   },
   "source": [
    "**Approche de segmentation de votre choix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G16gV7iroL_c"
   },
   "outputs": [],
   "source": [
    "def article_tokenize_other(text):\n",
    "    if type(text)!= str:\n",
    "        raise Exception(\"The function takes a string as input data\")\n",
    "    else:\n",
    "        # A COMPLETER\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sdbG5LEfoMCi"
   },
   "source": [
    "Tester votre méthode sur l'article donné ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WyIG3vpDoMCj"
   },
   "outputs": [],
   "source": [
    "text = 'UNITED NATIONS POTENT PYGMY IN MANHATTAN LAST WEEK, THE U.N .GREW TO A TOTAL OF 113 MEMBERS WITH THE ADMISSION OF THE NEWLYINDEPENDENT STATES OF ZANZIBAR AND KENYA . ZANZIBAR CONSISTS OF TWOSMALL ISLANDS IN THE INDIAN OCEAN, WITH A TOTAL POPULATION OF 310,000,OR ABOUT THAT OF OMAHA . NEVERTHELESS, ZANZIBAR HAS ONE VOTE IN THEGENERAL ASSEMBLY, AND IS THUS EQUAL IN VOTING POWER WITH SUCH NUCLEARGIANTS AS THE SOVIET UNION AND THE U.S .'\n",
    " \n",
    "# A completer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FHWUDrooMCm"
   },
   "source": [
    "**Segmentation de l'ensemble du corpus en mots**\n",
    "\n",
    "En appliquant vos différentes fonctions de segmentation sur différents exemples, vous devriez pouvoir effectuer une comparaison qualitative des différentes approches de segmentation et ainsi choisir la méthode de segmentation la plus appropriée pour le corpus TIME. \n",
    "Appliquer ensuite la méthode choisie à l'ensemble des documents du corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRDreFv8oMCn"
   },
   "outputs": [],
   "source": [
    "# a completer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1R3K4pNNLWm0"
   },
   "source": [
    "#### 2 . Filtrage des mots vides\n",
    "\n",
    "Ecrivez une fonction `article_remove_stop_words(text_tokens,stop_word_file)` qui prend respectivement en paramètres une liste de tokens puis une liste de mots vides et qui aura pour action de supprimer de la liste de tokens les mots vides fournis. Cette fonction devra renvoyer la nouvelle liste résultant de ce filtrage. On procédera ici aussi de deux manières :\n",
    "\n",
    " + Dans une première approche, vous prendrez les mots vides fournis avec la collection dans le fichier [TIME.STP](./Data/TIME.STP).\n",
    " + Dans une deuxième approche, vous constituerez vous même votre liste de mots vides en prenant par exemples les n=200 mots les plus fréquents de la collection. Vous pourrez vous même décider d'un bon choix de n.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h-Lm0eaMoMCs"
   },
   "source": [
    "**Approche 1 : creation d'une liste de mots vides en étudiant la fréquence des mots**\n",
    "\n",
    "Ecrivez une fonction qui permet de compter le nombre d'occurences de chaque token dans la collection. Vous pouvez pour cela, mais sans obligation, faire appel à la collection python [`Counter`](https://pymotw.com/2/collections/counter.html) de la bibliothèque standard de python qui permet de compter le nombre d'occurrences des éléments d'une collection et donc l'interface est très proche de celle des dictionnaires.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hji_yVFqoMCu"
   },
   "outputs": [],
   "source": [
    "def count_frequency(collection):\n",
    "    # A completer\n",
    "    return tokens_count\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrJJUqDgoMCx"
   },
   "source": [
    "Selectionner les n tokens les plus fréquents dans la collection et sauvegarder les dans une liste que l'on considèrera dans la suite comme une liste de mots-vides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ykvSlDF1oMCy"
   },
   "outputs": [],
   "source": [
    "# A completer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EEhh_Kr_oMC3"
   },
   "source": [
    "Visualiser sous la forme d'un histogramme les 25 tokens les plus fréquents de la collection. On utilisera pour cela les bibiothèques [matplolib](https://matplotlib.org/) et [seaborn](https://seaborn.pydata.org/) qu'il vous faudra installer (si ce n'est pas déja le cas) et importer au préalable. Le code ci-dessous vous montre un exemple de code qui affiche un histogramme de manière horizontale des 30 éléments les plus fréquents d'une collection de type `Counter` et que vous pouvez réutiliser, en l'adaptant, pour répondre à la question demandée. L'exemple est donné sur un article et non sur un  corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OG42g2DLoMC4",
    "outputId": "df85d986-c70a-42ae-e695-c75b8f96cbb1"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "text = 'UNITED NATIONS POTENT PYGMY IN MANHATTAN LAST WEEK, THE U.N .GREW TO A TOTAL OF 113 MEMBERS WITH THE ADMISSION OF THE NEWLYINDEPENDENT STATES OF ZANZIBAR AND KENYA . ZANZIBAR CONSISTS OF TWOSMALL ISLANDS IN THE INDIAN OCEAN, WITH A TOTAL POPULATION OF 310,000,OR ABOUT THAT OF OMAHA . NEVERTHELESS, ZANZIBAR HAS ONE VOTE IN THEGENERAL ASSEMBLY, AND IS THUS EQUAL IN VOTING POWER WITH SUCH NUCLEARGIANTS AS THE SOVIET UNION AND THE U.S .'\n",
    "tokens = text.split()\n",
    "count_tokens = collections.Counter(tokens)\n",
    "\n",
    "corpus_common_words = [word[0] for word in count_tokens.most_common(30)]\n",
    "corpus_common_counts = [word[1] for word in count_tokens.most_common(30)]\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "\n",
    "sns.barplot(x=corpus_common_counts,y=corpus_common_words)\n",
    "plt.title('Most Common Tokens in the article');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CBkNzL20oMC7"
   },
   "outputs": [],
   "source": [
    "# A completer : code pour visualiser les n tokens les plus frequents de la collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MUISWao0oMC_"
   },
   "source": [
    "Ecrire un programme permettant de comparer la liste obtenue avec les mots fréquents et la liste de mots vides fournie dans le fichier [TIME.STP](./Data/TIME.STP). Il vous faudra bien sûr d'abord charger le fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qhgLXeoxoMC_"
   },
   "outputs": [],
   "source": [
    "# A completer : comparaison des deux listes de mots vides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dLXq1i8MoMDF"
   },
   "source": [
    "Ecrire une fonction `remove_stop_words` qui prend en entrée une collection segmentée puis la liste des mots vides et qui renvoie la collection segmentée et filtrée(c'est à dire sous la forme d'un ensemble de documents segmentés et privés des mots vides). Pour rappel, dans ce TP, nous utilisons une représentation de la collection sous la forme d'un dictionnaire dont les clés sont les identifiants uniques des documents (`doc_id`) et les valeurs sont une liste des mots ou tokens du document. Nous pouvons travailler avec ce type de représentation car notre collection est de petite taille. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jFFHd1nILWm2"
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(collection ,stop_word_file):\n",
    "    # TO COMPLETE\n",
    "    return collection_filtered\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tRALQaTuoMDJ"
   },
   "source": [
    "Appliquer la fonction précédente avec les deux listes de mots vides proposées (liste des n mots les plus fréquents et liste des mots vides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DF_92h8hoMDK"
   },
   "outputs": [],
   "source": [
    "# A completer : application des opérations de filtrage à la collection TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n1nR14NSoMDN"
   },
   "source": [
    "Comparer les collections segmentées ainsi obtenues par exemple en comparant la taille (en nombre de tokens) après application des différentes étapes de filtrage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Y3phvxHoMDO"
   },
   "outputs": [],
   "source": [
    "# A completer : comparaison des méthodes de filtrage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_wY4i9dboMDR"
   },
   "source": [
    "Visualiser maintenant, toujours sous la forme d'un histogramme les 25 tokens les plus fréquents de la collection filtrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tykXnr8u-mIn"
   },
   "outputs": [],
   "source": [
    "# A completer : Visualisation sous la forme d'un histogramme les 25 tokens les plus fréquents de la collection filtrée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Xi_p4Y8LWm9"
   },
   "source": [
    "#### 3 . Normalisation des termes\n",
    "\n",
    "Il s'agit ici d'écrire les fonctions permettant d'apppliquer les étapes de normalisation (lemmatisation et racinisation) vues en cours et qui permettent de réduire la taille du vocabulaire. Pour rappel, cette étape consiste à représenter les mots par leur forme canonique ou leur racine afin de ne conserver que le sens des mots utilisés et de reduire ainsi la taille du vocabulaire de représentation. Ces opérations sont souvent assez complexes et nous allons pour cela utiliser les fonctions du package [`stem`](https://www.nltk.org/api/nltk.stem.html) de nltk.\n",
    "\n",
    "**Racinisation**\n",
    "\n",
    "Pour la racinisation, nous allons utiliser l'algorithme de Porter mentionné en cours et dont une implémentation est fournie dans le package `stem`. Compléter le code ci-dessous pour appliquer l'algorithme de Porter à votre collection. Attention, la fonction `stem` s'applique sur un mot ou token donné.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uEEe9zlEoMDT"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def collection_stemming(segmented_collection):\n",
    "    stemmer = PorterStemmer () # initialisation d'un stemmer\n",
    "    # à completer\n",
    "    return stemmed_collection\n",
    "\n",
    "# test sur la collection TIME filtrée des mots vides\n",
    "\n",
    "# à completer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hGNjGMU_oMDW"
   },
   "source": [
    "**Lemmatisation**\n",
    "\n",
    "Pour la lemmatisation, nous allons utiliser le `WordNetLemmatizer` du package `stem` de nltk. Comme précedemment, complèter le code ci-dessous pour l'appliquer à votre collection segmentée et privée des mots vides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gCH8nuj7oMDZ"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def collection_lemmatize(segmented_collection):\n",
    "    stemmer = WordNetLemmatizer () # initialisation d'un lemmatiseur\n",
    "    # à completer\n",
    "    return lemmatized_collection\n",
    "\n",
    "# test sur la collection TIME filtrée des mots vides\n",
    "\n",
    "# à completer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nnqd-XdRoMDc"
   },
   "source": [
    "Prenez le temps d'observer le résultat de ces opérations sur les mots de la collection TIME pour vous faire une idée sur l'impact que ces opérations peuvent avoir sur le processus d'indexation et de recherche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_lk5AL8OoMDe"
   },
   "source": [
    "#### 4 . Extraction du vocabulaire\n",
    "\n",
    "Nous arrivons maintenant à la dernière étape de cette chaîne de traitements linguistiques qui consiste à extraire le vocabulaire de termes résultant de cette suite de traitements. C'est une étape assez simple à mettre en oeuvre (c.f. travail précédent sur la fréquence des mots). Ici, nous cherchons les termes **uniques** et nous pouvons donc utiliser le type [`set`](https://docs.python.org/3/tutorial/datastructures.html#sets) prédéfini dans python et qui est une collection non-ordonnée d’objets **uniques et immuables**. \n",
    "Le code ci-dessous vous montre un exemple d'utilisation de cette structure de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HoAjtgjjoMDf",
    "outputId": "a1175449-64c2-4d43-e259-b65b53fa6c9f"
   },
   "outputs": [],
   "source": [
    "a = set()\n",
    "a.update(['bon'])\n",
    "a.update(['sur'])\n",
    "a.update(['au'])\n",
    "a.update(['sur'])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZ0OqEteoMDi"
   },
   "source": [
    "Completer le code ci-dessous permettant de construire le vocabulaire de termes d'index de votre collection pré-processée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8URg1qh6oMDj"
   },
   "outputs": [],
   "source": [
    "def extract_indexation_vocabulary(processed_collection):\n",
    "    vocabulary = set()\n",
    "    # à completer\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TYwwpYiJ-mI6"
   },
   "source": [
    "**Cette étape nous permet de voir qu'il y a beaucoup de tokens de type nombre, d'ailleurs mal segmentés. Il pourrait être par exemple nécessaire de supprimer les nombres des tokens en reprenant les différentes fonctions précédentes. Vous pouvez pour cela vous appuyer sur la fonction ci-dessous**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5bOhhxxW-mI7"
   },
   "outputs": [],
   "source": [
    "def vocabulary_cleaning(vocabulary):\n",
    "    clean_vocabulary = []\n",
    "    for token in vocabulary:\n",
    "        if re.match('[a-zA-Z]+',token) != None:\n",
    "            clean_vocabulary.append(token)\n",
    "    return sorted(clean_vocabulary)\n",
    "        \n",
    "print(vocabulary_cleaning(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uMRCZU6CLWnA"
   },
   "source": [
    "### Etape 3 : étude des lois de puissance du corpus.\n",
    "\n",
    "Vous allez ici vous intéresser à des informations statistiques sur votre corpus. Il faudra en particulier répondre aux différentes questions ci-dessous.\n",
    "\n",
    "\n",
    "#### Question 1 : Quel est le nombre de tokens dans le corpus ? \n",
    "\n",
    "Ecrire une fonction `count_tokens (collection)` permettant de compter le nombre de tokens dans votre collection. Vous pourrez par exemple comparer le nombre de tokens obtenus pour chacune des méthodes de segmentation implémentée précedemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QkygvmqJLWnC"
   },
   "outputs": [],
   "source": [
    "# a completer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "breMJgptLWnF"
   },
   "source": [
    "#### Question 2 : Quel est la taille du vocabulaire obtenu par l'application des traitements de l'étape 2 sur votre collection ?\n",
    "\n",
    "Ecrire une fonction `count_terms(collection)` permettant de compter le nombre de termes uniques obtenus après l'application des traitements de l'étape 2 sur la collection complète. Vous pourrez notamment mesurer l'impact des différentes étapes sur la taille du vocabulaire final en comparant les tailles obtenues à chaque étape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bXRZpnLILWnG"
   },
   "outputs": [],
   "source": [
    "# à compléter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kx8hRkhBLWnJ"
   },
   "source": [
    "#### Question 3 : Estimation sur la moitié de la collection ?\n",
    "    \n",
    "Calculer le nombre total de tokens et la taille du vocabulaire pour la moitié de la collection (211 premiers articles) et utiliser les résultats avec les deux précédents pour déterminer les paramètres k et b de la loi de Heap (c.f. cours 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LATPSAg0LWnL"
   },
   "outputs": [],
   "source": [
    "# à compléter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42CqpQa0LWnO"
   },
   "source": [
    "#### Question 4 : Et pour 1 million de tokens  ?\n",
    "\n",
    "Estimer la taille du vocabulaire pour une collection de 1 million de tokens pour votre collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "orO7xqSALWnP"
   },
   "outputs": [],
   "source": [
    "# à compléter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IM3nyAxqLWnT"
   },
   "source": [
    "#### Question 5 : Loi de Zipf\n",
    "\n",
    "Tracer le graphe fréquence (f) vs rang (r) pour tous les tokens de la collection. Tracer\n",
    "aussi le graphe log(f) vs log(r)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5kmsAnCKoMD7"
   },
   "outputs": [],
   "source": [
    "# à compléter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9t7yft5OLWnV"
   },
   "source": [
    "### Etape 4 : Construction de l'index inversé\n",
    "\n",
    "Nous avons maintenant tous les ingrédients nécessaires pour construire l'index inversé de la collection. Pour rappel, l'index inversé est une structure de données, fondamentale en recherche d'information, permettant de stocker l'information utile sous la forme d'un dictionnaire de termes dont la clé est un tuple formé du terme lui-même et de sa fréquence dans la collection (au sens `df`: nombre de documents dans lesquels le terme apparaît au moins une fois) et dont la valeur est une liste qui enregistre à minima les numéros des documents dans lesquels le terme apparait.\n",
    "\n",
    "\n",
    "<img src=\"./Images/iindex.png\" width=\"500\" height=\"500\" />\n",
    "\n",
    "Pour la collection TIME, l'algorithme ci-dessous peut être utilisé pour construire cet index.\n",
    "\n",
    "\n",
    "<img src=\"./Images/algorithm.png\" width=\"500\" height=\"500\" />\n",
    "\n",
    "Completer la fonction `build_inverted_index(collection,type_index)` ci-dessous qui permet de construire l'index inversé d'une collection donnée en paramètre.  Un second paramètre `type_index` vous permettra de préciser le type d'index à construire parmi les 3 vus en cours : index de documents (type = 1), index de fréquence (type = 2) et index de position (type =3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QC_4LuFKoMEK"
   },
   "outputs": [],
   "source": [
    "def build_inverted_index(collection,type_index):\n",
    "    # à complèter\n",
    "    return inverted_index\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wY0xLxqVoMEO"
   },
   "source": [
    "#### Sauvegarde de l'index inversé dans un fichier \n",
    "\n",
    "Il nous reste maintenant à sauvegarder l'index créé sous la forme d'un fichier texte selon le format ci-dessous :\n",
    "```\n",
    "Term1, df_1 | (doc_1, tf_doc1_Term1) (doc_4, tf_doc_4_Term1)\n",
    "Term2, df_2 | (doc_3, tf_doc3_Term2) \n",
    "...\n",
    "```\n",
    "\n",
    "Pour un index de position, le format sera le suivant :\n",
    "```\n",
    "Term1, df_1 | (doc_1, tf_doc1_Term1 ; pos1 pos2 ... posn) (doc_4, tf_doc_4_Term1; pos1 pos2 )\n",
    "Term2, df_2 | (doc_3, tf_doc3_Term2 ; pos1 pos2 ..posm) \n",
    "...\n",
    "```\n",
    "Ecrire une fonction permettant de sauvegarder votre index inversé sur disque sous la forme d'un fichier texte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y09e6U50oMEW"
   },
   "outputs": [],
   "source": [
    "# A compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BtRH2CWooMEZ"
   },
   "source": [
    "#### Chargement d'un index inversé\n",
    "\n",
    "Ecrire une fonction permettant de charger un index inversé sauvegardé sous la forme d'un fichier texte comme un dictionnaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YDYToBiNoMEa"
   },
   "outputs": [],
   "source": [
    "# A compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cRvMqv1noMEe"
   },
   "source": [
    "#### Taille et temps de construction de l'index\n",
    "\n",
    "Il s'agit ici de répondre aux questions suivantes :\n",
    "+ Quelle est la taille de l'index inversé obtenu en mémoire ? (indice : utilisation de `getsizeof`du module sys)\n",
    "+ Quelle est la taille de l'index inversé écrit sur disque ?\n",
    "+ Combien de temps au total prend la construction de votre index (indice : utilisation du module `time`) ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "43RmguxhoMEf"
   },
   "outputs": [],
   "source": [
    "# A compléter"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab1_Indexation-Student.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
